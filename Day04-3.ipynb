{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "widespread-mechanics",
   "metadata": {},
   "source": [
    "### 출처: Won Joon Yoo, Introduction to Deep Learning for Natural Language Processing, Wikidocs</br>\n",
    "### https://wikidocs.net/book/2155"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "medieval-cologne",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import urllib.request\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "infrared-filename",
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"http://www.gutenberg.org/files/11/11-0.txt\", filename=\"11-0.txt\")\n",
    "f = open('11-0.txt', 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "latter-entry",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines=[]\n",
    "for line in f: # 데이터를 한 줄씩 읽는다.\n",
    "    line=line.strip() # strip()을 통해 \\r, \\n을 제거한다.\n",
    "    line=line.lower() # 소문자화.\n",
    "    # 아스키로 바꾸는데 인식 안 되는 것은 모두 제거\n",
    "    line=line.decode('ascii', 'ignore') # \\xe2\\x80\\x99 등과 같은 바이트 열 제거\n",
    "    if len(line) > 0:\n",
    "        lines.append(line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "continuous-passenger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the project gutenberg ebook of alices adventures in wonderland, by lewis carroll',\n",
       " 'this ebook is for the use of anyone anywhere in the united states and most',\n",
       " 'other parts of the world at no cost and with almost no restrictions']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한 문장으로 분리된 것이 아님을 확인\n",
    "lines[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "radio-ordering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자열의 길이 또는 총 글자의 개수: 159821\n"
     ]
    }
   ],
   "source": [
    "# 모두 한 문장으로 만들기\n",
    "text = ' '.join(lines)\n",
    "print('문자열의 길이 또는 총 글자의 개수: %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "blank-criticism",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the project gutenberg ebook of alices adventures in wonderland, by lewis carroll this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "neutral-country",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "글자 집합의 크기 : 57\n"
     ]
    }
   ],
   "source": [
    "# set을 이용해서 중복 문자 제거\n",
    "char_vocab = sorted(list(set(text)))\n",
    "vocab_size=len(char_vocab)\n",
    "print ('글자 집합의 크기 : {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "little-cathedral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " '!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " ']',\n",
       " '_',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 소문자만 있는 것 확인\n",
    "char_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "jewish-contamination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, '!': 1, '\"': 2, '#': 3, '$': 4, '%': 5, \"'\": 6, '(': 7, ')': 8, '*': 9, ',': 10, '-': 11, '.': 12, '/': 13, '0': 14, '1': 15, '2': 16, '3': 17, '4': 18, '5': 19, '6': 20, '7': 21, '8': 22, '9': 23, ':': 24, ';': 25, '?': 26, '@': 27, '[': 28, ']': 29, '_': 30, 'a': 31, 'b': 32, 'c': 33, 'd': 34, 'e': 35, 'f': 36, 'g': 37, 'h': 38, 'i': 39, 'j': 40, 'k': 41, 'l': 42, 'm': 43, 'n': 44, 'o': 45, 'p': 46, 'q': 47, 'r': 48, 's': 49, 't': 50, 'u': 51, 'v': 52, 'w': 53, 'x': 54, 'y': 55, 'z': 56}\n"
     ]
    }
   ],
   "source": [
    "# 사전 만들기\n",
    "char_to_index = dict((c, i) for i, c in enumerate(char_vocab)) # 글자에 고유한 정수 인덱스 부여\n",
    "print(char_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "automated-theorem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 순서 반대인 사전 만들기\n",
    "index_to_char={}\n",
    "for key, value in char_to_index.items():\n",
    "    index_to_char[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "incorporate-chuck",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ' ', 1: '!', 2: '\"', 3: '#', 4: '$', 5: '%', 6: \"'\", 7: '(', 8: ')', 9: '*', 10: ',', 11: '-', 12: '.', 13: '/', 14: '0', 15: '1', 16: '2', 17: '3', 18: '4', 19: '5', 20: '6', 21: '7', 22: '8', 23: '9', 24: ':', 25: ';', 26: '?', 27: '@', 28: '[', 29: ']', 30: '_', 31: 'a', 32: 'b', 33: 'c', 34: 'd', 35: 'e', 36: 'f', 37: 'g', 38: 'h', 39: 'i', 40: 'j', 41: 'k', 42: 'l', 43: 'm', 44: 'n', 45: 'o', 46: 'p', 47: 'q', 48: 'r', 49: 's', 50: 't', 51: 'u', 52: 'v', 53: 'w', 54: 'x', 55: 'y', 56: 'z'}\n"
     ]
    }
   ],
   "source": [
    "print(index_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sexual-columbus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 샘플의 수 : 2663\n"
     ]
    }
   ],
   "source": [
    "# 임의로 문장의 길이를 60으로 함\n",
    "seq_length = 60 # 문장의 길이를 60으로 한다.\n",
    "n_samples = int(np.floor((len(text) - 1) / seq_length)) # 문자열을 60등분한다. 그러면 즉, 총 샘플의 개수\n",
    "print ('문장 샘플의 수 : {}'.format(n_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sunset-glory",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = []\n",
    "train_y = []\n",
    "\n",
    "for i in range(n_samples): # 2,646번 수행\n",
    "    X_sample = text[i * seq_length: (i + 1) * seq_length]\n",
    "    # 0:60 -> 60:120 -> 120:180로 loop를 돌면서 문장 샘플을 1개씩 가져온다.\n",
    "    X_encoded = [char_to_index[c] for c in X_sample] # 하나의 문장 샘플에 대해서 정수 인코딩\n",
    "    train_X.append(X_encoded)\n",
    "\n",
    "    y_sample = text[i * seq_length + 1: (i + 1) * seq_length + 1] # 오른쪽으로 1칸 쉬프트한다.\n",
    "    y_encoded = [char_to_index[c] for c in y_sample]\n",
    "    train_y.append(y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "married-intranet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50, 38, 35, 0, 46, 48, 45, 40, 35, 33, 50, 0, 37, 51, 50, 35, 44, 32, 35, 48, 37, 0, 35, 32, 45, 45, 41, 0, 45, 36, 0, 31, 42, 39, 33, 35, 49, 0, 31, 34, 52, 35, 44, 50, 51, 48, 35, 49, 0, 39, 44, 0, 53, 45, 44, 34, 35, 48, 42, 31]\n"
     ]
    }
   ],
   "source": [
    "print(train_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hybrid-moisture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38, 35, 0, 46, 48, 45, 40, 35, 33, 50, 0, 37, 51, 50, 35, 44, 32, 35, 48, 37, 0, 35, 32, 45, 45, 41, 0, 45, 36, 0, 31, 42, 39, 33, 35, 49, 0, 31, 34, 52, 35, 44, 50, 51, 48, 35, 49, 0, 39, 44, 0, 53, 45, 44, 34, 35, 48, 42, 31, 44]\n"
     ]
    }
   ],
   "source": [
    "print(train_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "modern-eight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원-핫 인코딩\n",
    "train_X = to_categorical(train_X)\n",
    "train_y = to_categorical(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "reliable-terminology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X의 크기(shape) : (2663, 60, 57)\n",
      "train_y의 크기(shape) : (2663, 60, 57)\n"
     ]
    }
   ],
   "source": [
    "print('train_X의 크기(shape) : {}'.format(train_X.shape)) # 원-핫 인코딩\n",
    "print('train_y의 크기(shape) : {}'.format(train_y.shape)) # 원-핫 인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-nightlife",
   "metadata": {},
   "source": [
    "### 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "typical-minister",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, TimeDistributed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "respected-earth",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(None, train_X.shape[2]), return_sequences=True))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(vocab_size, activation='softmax')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "plastic-behalf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "84/84 - 14s - loss: 3.0762 - accuracy: 0.1821\n",
      "Epoch 2/80\n",
      "84/84 - 14s - loss: 2.7170 - accuracy: 0.2516\n",
      "Epoch 3/80\n",
      "84/84 - 14s - loss: 2.3796 - accuracy: 0.3306\n",
      "Epoch 4/80\n",
      "84/84 - 14s - loss: 2.2494 - accuracy: 0.3597\n",
      "Epoch 5/80\n",
      "84/84 - 14s - loss: 2.1343 - accuracy: 0.3890\n",
      "Epoch 6/80\n",
      "84/84 - 14s - loss: 2.0551 - accuracy: 0.4098\n",
      "Epoch 7/80\n",
      "84/84 - 14s - loss: 1.9861 - accuracy: 0.4265\n",
      "Epoch 8/80\n",
      "84/84 - 14s - loss: 1.9281 - accuracy: 0.4406\n",
      "Epoch 9/80\n",
      "84/84 - 14s - loss: 1.8748 - accuracy: 0.4553\n",
      "Epoch 10/80\n",
      "84/84 - 14s - loss: 1.8270 - accuracy: 0.4698\n",
      "Epoch 11/80\n",
      "84/84 - 14s - loss: 1.7839 - accuracy: 0.4821\n",
      "Epoch 12/80\n",
      "84/84 - 14s - loss: 1.7430 - accuracy: 0.4927\n",
      "Epoch 13/80\n",
      "84/84 - 14s - loss: 1.7039 - accuracy: 0.5040\n",
      "Epoch 14/80\n",
      "84/84 - 14s - loss: 1.6654 - accuracy: 0.5134\n",
      "Epoch 15/80\n",
      "84/84 - 14s - loss: 1.6307 - accuracy: 0.5236\n",
      "Epoch 16/80\n",
      "84/84 - 14s - loss: 1.5960 - accuracy: 0.5328\n",
      "Epoch 17/80\n",
      "84/84 - 14s - loss: 1.5628 - accuracy: 0.5416\n",
      "Epoch 18/80\n",
      "84/84 - 14s - loss: 1.5289 - accuracy: 0.5505\n",
      "Epoch 19/80\n",
      "84/84 - 14s - loss: 1.4975 - accuracy: 0.5585\n",
      "Epoch 20/80\n",
      "84/84 - 14s - loss: 1.4675 - accuracy: 0.5661\n",
      "Epoch 21/80\n",
      "84/84 - 14s - loss: 1.4376 - accuracy: 0.5749\n",
      "Epoch 22/80\n",
      "84/84 - 14s - loss: 1.4078 - accuracy: 0.5823\n",
      "Epoch 23/80\n",
      "84/84 - 14s - loss: 1.3792 - accuracy: 0.5907\n",
      "Epoch 24/80\n",
      "84/84 - 14s - loss: 1.3490 - accuracy: 0.5990\n",
      "Epoch 25/80\n",
      "84/84 - 14s - loss: 1.3213 - accuracy: 0.6070\n",
      "Epoch 26/80\n",
      "84/84 - 14s - loss: 1.2920 - accuracy: 0.6158\n",
      "Epoch 27/80\n",
      "84/84 - 14s - loss: 1.2653 - accuracy: 0.6233\n",
      "Epoch 28/80\n",
      "84/84 - 14s - loss: 1.2378 - accuracy: 0.6305\n",
      "Epoch 29/80\n",
      "84/84 - 14s - loss: 1.2103 - accuracy: 0.6383\n",
      "Epoch 30/80\n",
      "84/84 - 14s - loss: 1.1799 - accuracy: 0.6480\n",
      "Epoch 31/80\n",
      "84/84 - 14s - loss: 1.1534 - accuracy: 0.6562\n",
      "Epoch 32/80\n",
      "84/84 - 14s - loss: 1.1245 - accuracy: 0.6639\n",
      "Epoch 33/80\n",
      "84/84 - 14s - loss: 1.0967 - accuracy: 0.6720\n",
      "Epoch 34/80\n",
      "84/84 - 14s - loss: 1.0680 - accuracy: 0.6816\n",
      "Epoch 35/80\n",
      "84/84 - 14s - loss: 1.0376 - accuracy: 0.6899\n",
      "Epoch 36/80\n",
      "84/84 - 14s - loss: 1.0107 - accuracy: 0.6987\n",
      "Epoch 37/80\n",
      "84/84 - 14s - loss: 0.9813 - accuracy: 0.7069\n",
      "Epoch 38/80\n",
      "84/84 - 14s - loss: 0.9535 - accuracy: 0.7165\n",
      "Epoch 39/80\n",
      "84/84 - 14s - loss: 0.9243 - accuracy: 0.7247\n",
      "Epoch 40/80\n",
      "84/84 - 14s - loss: 0.8982 - accuracy: 0.7328\n",
      "Epoch 41/80\n",
      "84/84 - 14s - loss: 0.8704 - accuracy: 0.7420\n",
      "Epoch 42/80\n",
      "84/84 - 14s - loss: 0.8430 - accuracy: 0.7504\n",
      "Epoch 43/80\n",
      "84/84 - 14s - loss: 0.8150 - accuracy: 0.7590\n",
      "Epoch 44/80\n",
      "84/84 - 14s - loss: 0.7897 - accuracy: 0.7669\n",
      "Epoch 45/80\n",
      "84/84 - 14s - loss: 0.7595 - accuracy: 0.7763\n",
      "Epoch 46/80\n",
      "84/84 - 14s - loss: 0.7373 - accuracy: 0.7831\n",
      "Epoch 47/80\n",
      "84/84 - 14s - loss: 0.7140 - accuracy: 0.7907\n",
      "Epoch 48/80\n",
      "84/84 - 14s - loss: 0.6872 - accuracy: 0.7992\n",
      "Epoch 49/80\n",
      "84/84 - 14s - loss: 0.6640 - accuracy: 0.8072\n",
      "Epoch 50/80\n",
      "84/84 - 14s - loss: 0.6401 - accuracy: 0.8149\n",
      "Epoch 51/80\n",
      "84/84 - 14s - loss: 0.6173 - accuracy: 0.8220\n",
      "Epoch 52/80\n",
      "84/84 - 14s - loss: 0.5932 - accuracy: 0.8304\n",
      "Epoch 53/80\n",
      "84/84 - 14s - loss: 0.5716 - accuracy: 0.8365\n",
      "Epoch 54/80\n",
      "84/84 - 14s - loss: 0.5510 - accuracy: 0.8435\n",
      "Epoch 55/80\n",
      "84/84 - 14s - loss: 0.5269 - accuracy: 0.8524\n",
      "Epoch 56/80\n",
      "84/84 - 14s - loss: 0.5069 - accuracy: 0.8585\n",
      "Epoch 57/80\n",
      "84/84 - 14s - loss: 0.4904 - accuracy: 0.8629\n",
      "Epoch 58/80\n",
      "84/84 - 14s - loss: 0.4700 - accuracy: 0.8702\n",
      "Epoch 59/80\n",
      "84/84 - 14s - loss: 0.4513 - accuracy: 0.8753\n",
      "Epoch 60/80\n",
      "84/84 - 14s - loss: 0.4323 - accuracy: 0.8827\n",
      "Epoch 61/80\n",
      "84/84 - 14s - loss: 0.4183 - accuracy: 0.8861\n",
      "Epoch 62/80\n",
      "84/84 - 14s - loss: 0.4013 - accuracy: 0.8914\n",
      "Epoch 63/80\n",
      "84/84 - 14s - loss: 0.3852 - accuracy: 0.8968\n",
      "Epoch 64/80\n",
      "84/84 - 14s - loss: 0.3661 - accuracy: 0.9030\n",
      "Epoch 65/80\n",
      "84/84 - 14s - loss: 0.3516 - accuracy: 0.9080\n",
      "Epoch 66/80\n",
      "84/84 - 14s - loss: 0.3372 - accuracy: 0.9124\n",
      "Epoch 67/80\n",
      "84/84 - 14s - loss: 0.3217 - accuracy: 0.9176\n",
      "Epoch 68/80\n",
      "84/84 - 14s - loss: 0.3107 - accuracy: 0.9201\n",
      "Epoch 69/80\n",
      "84/84 - 14s - loss: 0.3023 - accuracy: 0.9223\n",
      "Epoch 70/80\n",
      "84/84 - 14s - loss: 0.2855 - accuracy: 0.9285\n",
      "Epoch 71/80\n",
      "84/84 - 14s - loss: 0.2788 - accuracy: 0.9298\n",
      "Epoch 72/80\n",
      "84/84 - 14s - loss: 0.2677 - accuracy: 0.9329\n",
      "Epoch 73/80\n",
      "84/84 - 14s - loss: 0.2621 - accuracy: 0.9342\n",
      "Epoch 74/80\n",
      "84/84 - 14s - loss: 0.2556 - accuracy: 0.9360\n",
      "Epoch 75/80\n",
      "84/84 - 14s - loss: 0.2430 - accuracy: 0.9396\n",
      "Epoch 76/80\n",
      "84/84 - 14s - loss: 0.2339 - accuracy: 0.9423\n",
      "Epoch 77/80\n",
      "84/84 - 14s - loss: 0.2270 - accuracy: 0.9440\n",
      "Epoch 78/80\n",
      "84/84 - 14s - loss: 0.2143 - accuracy: 0.9484\n",
      "Epoch 79/80\n",
      "84/84 - 14s - loss: 0.2018 - accuracy: 0.9518\n",
      "Epoch 80/80\n",
      "84/84 - 14s - loss: 0.1956 - accuracy: 0.9530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcb4033b220>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_X, train_y, epochs=80, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "intended-setting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(model, length):\n",
    "    ix = [np.random.randint(vocab_size)] # 글자에 대한 랜덤 인덱스 생성\n",
    "    y_char = [index_to_char[ix[-1]]] # 랜덤 익덱스로부터 글자 생성\n",
    "    print(ix[-1],'번 글자',y_char[-1],'로 예측을 시작!')\n",
    "    X = np.zeros((1, length, vocab_size)) # (1, length, 55) 크기의 X 생성. 즉, LSTM의 입력 시퀀스 생성\n",
    "\n",
    "    for i in range(length):\n",
    "        X[0][i][ix[-1]] = 1 # X[0][i][예측한 글자의 인덱스] = 1, 즉, 예측 글자를 다음 입력 시퀀스에 추가\n",
    "        print(index_to_char[ix[-1]], end=\"\")\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        y_char.append(index_to_char[ix[-1]])\n",
    "    return ('').join(y_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "surprising-academy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 번 글자 c 로 예측을 시작!\n",
      "ce, and he had then fan indedddy in a complewion till at tell without a bit thing is, that the pool "
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ce, and he had then fan indedddy in a complewion till at tell without a bit thing is, that the pool w'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-defense",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
